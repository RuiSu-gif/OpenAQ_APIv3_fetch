{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAQ Hourly Data Downloader\n",
    "\n",
    "This notebook fetches hourly PM2.5 and ozone measurements from the OpenAQ v3 API. Update the configuration below to reference your country metadata CSV before running the main workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ===================  User Configuration  ===================\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# 1) OpenAQ authentication (v3 requires an API key)\n",
    "API_KEY = os.getenv(\"OPENAQ_API_KEY\")  # set this environment variable before running\n",
    "\n",
    "# 1.1) Proxy control\n",
    "DISABLE_ENV_PROXIES = True  # Set to False if corporate proxies are required\n",
    "\n",
    "# 2) Country metadata (CSV file used to map regions to OpenAQ)\n",
    "COUNTRY_CSV_PATH = \"E:/Github_project/rfasst_test/data/Regions.csv\"\n",
    "COUNTRY_CSV_COLUMNS = {\n",
    "    \"country_name\": \"country_name\",  # required; CSV column name for country labels\n",
    "    \"iso3\": \"iso3\",                  # required; CSV column name for ISO3 codes\n",
    "    \"fasst_region\": \"fasst_region\",  # optional; set to None if the CSV does not contain this column\n",
    "}\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), \"output\")  # per-country CSV files will be written here\n",
    "\n",
    "# 3) Sampling strategy\n",
    "MAX_LOCATIONS_PER_COUNTRY = 200  # <=0 means no limit\n",
    "\n",
    "# 4) Country filtering (optional)\n",
    "INCLUDE_COUNTRIES = []  # limit run to these ISO2/ISO3 codes or names\n",
    "EXCLUDE_COUNTRIES = []  # exclude these ISO2/ISO3 codes or names\n",
    "\n",
    "# 5) Rate limiting and retry behaviour (align with OpenAQ v3 limits)\n",
    "MAX_LIMIT = 1000\n",
    "REQUESTS_PER_MIN = 55\n",
    "REQUESTS_PER_HOUR = 1900\n",
    "REQUEST_SLEEP = 0.10\n",
    "RETRY_MAX_TRIES = 6\n",
    "RETRY_BACKOFF_BASE = 0.5\n",
    "RETRY_BACKOFF_FACTOR = 2.0\n",
    "RETRY_AFTER_CAP = 90\n",
    "PER_COUNTRY_PAUSE_SEC = 30\n",
    "\n",
    "# 6) Optional geographic filters\n",
    "WITHIN_WKT = None\n",
    "WITHIN_RADIUS_KM = None\n",
    "BBOX = None  # [minLon, minLat, maxLon, maxLat]\n",
    "\n",
    "# 7) Hourly data chunking parameters\n",
    "HOURS_CHUNK_DAYS = 90\n",
    "MAX_CHUNK_SPLIT_DEPTH = 3\n",
    "PAGE_LIMITS_TRY = [1000, 500, 200]\n",
    "REQUEST_TIMEOUT = 30\n",
    "\n",
    "# 8) Hourly data time window (UTC)\n",
    "DATETIME_FROM = \"2020-01-01T00:00:00Z\"\n",
    "DATETIME_TO = \"2024-12-31T23:59:59Z\"\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import re, time, math\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openaq import OpenAQ\n",
    "\n",
    "# --- HTTP 会话直连 v3，用于 /v3/sensors/{id}/hours（更可控的超时/重试） ---\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "BASE_URL = \"https://api.openaq.org/v3\"\n",
    "\n",
    "def _build_http_session(api_key: str) -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    if api_key:\n",
    "        headers[\"X-API-Key\"] = api_key\n",
    "    s.headers.update(headers)\n",
    "    retry = Retry(\n",
    "        total=RETRY_MAX_TRIES,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=(408, 429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"GET\"]),\n",
    "        raise_on_status=False,\n",
    "        respect_retry_after_header=True,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "def _maybe_disable_env_proxies():\n",
    "    if not DISABLE_ENV_PROXIES:\n",
    "        return\n",
    "    for k in [\"HTTP_PROXY\",\"HTTPS_PROXY\",\"ALL_PROXY\",\"NO_PROXY\",\"http_proxy\",\"https_proxy\",\"all_proxy\",\"no_proxy\"]:\n",
    "        os.environ.pop(k, None)\n",
    "\n",
    "_RATE_LIMIT_RE = re.compile(r\"Limit resets in\\s+(\\d+)\\s+seconds\", re.I)\n",
    "\n",
    "def _classify_exc(e: Exception) -> str:\n",
    "    s = str(e)\n",
    "    if \"Rate limit exceeded\" in s or \"429\" in s:\n",
    "        return \"rate\"\n",
    "    if \"404\" in s or \"Not Found\" in s:\n",
    "        return \"notfound\"\n",
    "    if any(x in s for x in [\"500\",\"502\",\"503\",\"504\",\"Internal Server Error\",\"Bad Gateway\"]):\n",
    "        return \"server\"\n",
    "    if \"timeout\" in s.lower() or \"timed out\" in s.lower():\n",
    "        return \"timeout\"\n",
    "    return \"other\"\n",
    "\n",
    "def _parse_rate_reset_from_resp(resp: requests.Response) -> Optional[int]:\n",
    "    if resp is not None and hasattr(resp, \"headers\"):\n",
    "        ra = resp.headers.get(\"Retry-After\")\n",
    "        if ra:\n",
    "            try:\n",
    "                return min(int(ra), RETRY_AFTER_CAP)\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def _parse_rate_reset(e: Exception) -> Optional[int]:\n",
    "    m = _RATE_LIMIT_RE.search(str(e))\n",
    "    if m:\n",
    "        try:\n",
    "            return min(int(m.group(1)), RETRY_AFTER_CAP)\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        resp = getattr(e, \"response\", None)\n",
    "        if resp:\n",
    "            return _parse_rate_reset_from_resp(resp)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _sleep_with_backoff(attempt: int):\n",
    "    delay = RETRY_BACKOFF_BASE * (RETRY_BACKOFF_FACTOR ** max(0, attempt - 1))\n",
    "    time.sleep(delay)\n",
    "\n",
    "def _parse_iso_z(v: Optional[str]) -> Optional[str]:\n",
    "    if v is None or str(v).strip() == \"\" or str(v).strip().lower() == \"none\":\n",
    "        return None\n",
    "    try:\n",
    "        ts = pd.to_datetime(v, utc=True)\n",
    "        return ts.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, rpm: int = REQUESTS_PER_MIN, rph: int = REQUESTS_PER_HOUR):\n",
    "        self.min_interval = 60.0 / max(1, rpm)\n",
    "        self.hour_budget = rph\n",
    "        self.last_call_ts = 0.0\n",
    "        self.window_start = time.time()\n",
    "        self.used_this_hour = 0\n",
    "    def wait(self):\n",
    "        now = time.time()\n",
    "        since_last = now - self.last_call_ts\n",
    "        if since_last < self.min_interval:\n",
    "            time.sleep(self.min_interval - since_last)\n",
    "        self.last_call_ts = time.time()\n",
    "        now = time.time()\n",
    "        if now - self.window_start >= 3600:\n",
    "            self.window_start = now\n",
    "            self.used_this_hour = 0\n",
    "        self.used_this_hour += 1\n",
    "\n",
    "RL = RateLimiter()\n",
    "\n",
    "\n",
    "# ================= Country CSV 读取 & 国家映射 =================\n",
    "COUNTRY_NAME_ALIASES = {\n",
    "    \"KOREA; REPUBLIC OF\": \"Republic of Korea\",\n",
    "    \"CONGO;  Democratic Republic of (was Zaire)\": \"Democratic Republic of the Congo\",\n",
    "    \"TANZANIA;UNITED REPUBLIC OF\": \"Tanzania\",\n",
    "    \"IRAN (ISLAMIC REPUBLIC OF)\": \"Iran\",\n",
    "    \"VIET NAM\": \"Vietnam\",\n",
    "    \"SYRIAN ARAB REPUBLIC\": \"Syrian Arab Republic\",\n",
    "    \"LIBYAN ARAB JAMAHIRIYA\": \"Libya\",\n",
    "    \"CROATIA (local name: Hrvatska)\": \"Croatia\",\n",
    "    \"BOLIVIA\": \"Bolivia (Plurinational State of)\",\n",
    "    \"VENEZUELA\": \"Venezuela (Bolivarian Republic of)\",\n",
    "    \"MOLDOVA; REPUBLIC OF\": \"Republic of Moldova\",\n",
    "    \"MACAU\": \"Macao\",\n",
    "    \"PALESTINIAN TERRITORY; Occupied\": \"State of Palestine\",\n",
    "    \"KOREA; DEMOCRATIC PEOPLE'S REPUBLIC OF\": \"Democratic People's Republic of Korea\",\n",
    "}\n",
    "HISTORIC_ISO3_TO_ISO2 = {\"SCG\": [\"RS\",\"ME\"], \"ANT\": [\"CW\",\"SX\",\"BQ\"]}\n",
    "\n",
    "def load_regions_csv(csv_path: str, column_map: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"Load country metadata from a CSV file and normalise key columns.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Country CSV not found: {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    column_map = column_map or {}\n",
    "    required_keys = [\"country_name\", \"iso3\"]\n",
    "    optional_keys = [\"fasst_region\"]\n",
    "\n",
    "    rename_map = {}\n",
    "    for key in required_keys + optional_keys:\n",
    "        source = column_map.get(key) or key\n",
    "        if source not in df.columns:\n",
    "            if key in required_keys:\n",
    "                raise ValueError(f\"Column '{source}' required for '{key}' was not found in {csv_path}.\")\n",
    "            else:\n",
    "                df[key] = None\n",
    "                continue\n",
    "        if source != key:\n",
    "            rename_map[source] = key\n",
    "\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "    for key in optional_keys:\n",
    "        if key not in df.columns:\n",
    "            df[key] = None\n",
    "\n",
    "    df[\"country_name\"] = df[\"country_name\"].astype(str).str.strip()\n",
    "    if \"iso3\" in df.columns:\n",
    "        df[\"iso3\"] = df[\"iso3\"].astype(str).str.strip().str.upper()\n",
    "    df = df.dropna(subset=[\"country_name\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = (s or \"\").lower().strip()\n",
    "    s = re.sub(r\"[^\\w\\s\\-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def map_countries_to_openaq_ids(client: OpenAQ, regions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 获取 OpenAQ countries 列表\n",
    "    tries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            RL.wait()\n",
    "            oq_df = pd.DataFrame(client.countries.list(limit=MAX_LIMIT).dict()[\"results\"])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            tries += 1\n",
    "            cls = _classify_exc(e)\n",
    "            if cls == \"rate\":\n",
    "                reset = _parse_rate_reset(e) or 25\n",
    "                print(f\"[RATE] countries.list 限流，等待 {reset}s ...\")\n",
    "                time.sleep(reset + 1); continue\n",
    "            if tries >= RETRY_MAX_TRIES: raise\n",
    "            _sleep_with_backoff(tries)\n",
    "\n",
    "    oq_df[\"name_norm\"] = oq_df[\"name\"].map(_norm)\n",
    "    oq_df[\"code\"] = oq_df[\"code\"].astype(str).str.upper()\n",
    "    if \"code3\" not in oq_df.columns: oq_df[\"code3\"] = None\n",
    "\n",
    "    rows, misses = [], []\n",
    "    for _, r in regions_df.iterrows():\n",
    "        nm = str(r.get(\"country_name\",\"\")).strip()\n",
    "        iso3 = str(r.get(\"iso3\",\"\")).upper().strip()\n",
    "        fasst = r.get(\"fasst_region\")\n",
    "\n",
    "        if iso3 in HISTORIC_ISO3_TO_ISO2:\n",
    "            iso2s = HISTORIC_ISO3_TO_ISO2[iso3]\n",
    "            hits = oq_df[oq_df[\"code\"].isin(iso2s)]\n",
    "            if not hits.empty:\n",
    "                for _, best in hits.iterrows():\n",
    "                    rows.append({\n",
    "                        \"countries_id\": best[\"id\"],\n",
    "                        \"country_name\": best[\"name\"],\n",
    "                        \"country_code\": best[\"code\"],\n",
    "                        \"iso3\": best.get(\"code3\") or iso3,\n",
    "                        \"fasst_region\": fasst,\n",
    "                        \"country_name_req\": nm,\n",
    "                        \"iso3_req\": iso3,\n",
    "                    })\n",
    "                continue\n",
    "            else:\n",
    "                misses.append({\"country_name_req\": nm, \"iso3_req\": iso3}); continue\n",
    "\n",
    "        aliased = COUNTRY_NAME_ALIASES.get(nm, nm)\n",
    "        nm_norm = _norm(aliased) if aliased else None\n",
    "\n",
    "        hit = None\n",
    "        if iso3 and \"code3\" in oq_df and oq_df[\"code3\"].notna().any():\n",
    "            hit = oq_df[oq_df[\"code3\"] == iso3]\n",
    "        if (hit is None or hit.empty) and nm_norm:\n",
    "            hit = oq_df[oq_df[\"name_norm\"] == nm_norm]\n",
    "        if (hit is None or hit.empty) and nm_norm:\n",
    "            hit = oq_df[oq_df[\"name_norm\"].str.contains(rf\"^{re.escape(nm_norm)}\") |\n",
    "                        oq_df[\"name_norm\"].str.contains(rf\"{re.escape(nm_norm)}$\")]\n",
    "        if hit is not None and not hit.empty:\n",
    "            best = hit.iloc[0]\n",
    "            rows.append({\n",
    "                \"countries_id\": best[\"id\"],\n",
    "                \"country_name\": best[\"name\"],\n",
    "                \"country_code\": best[\"code\"],\n",
    "                \"iso3\": iso3 if iso3 else None,\n",
    "                \"fasst_region\": fasst,\n",
    "                \"country_name_req\": nm,\n",
    "                \"iso3_req\": iso3,\n",
    "            })\n",
    "        else:\n",
    "            misses.append({\"country_name_req\": nm, \"iso3_req\": iso3})\n",
    "\n",
    "    if misses:\n",
    "        print(\"未匹配到 OpenAQ 的国家（建议补充别名/校对 ISO3）：\")\n",
    "        for m in misses: print(\"  -\", m)\n",
    "\n",
    "    def _norm_any(x: str) -> str: return str(x or \"\").strip().upper()\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if INCLUDE_COUNTRIES:\n",
    "        inc = set(map(_norm_any, INCLUDE_COUNTRIES))\n",
    "        df = df[\n",
    "            df[\"country_code\"].map(_norm_any).isin(inc) |\n",
    "            df[\"country_name\"].map(_norm_any).isin(inc) |\n",
    "            df[\"iso3\"].map(_norm_any).isin(inc)\n",
    "        ]\n",
    "    if EXCLUDE_COUNTRIES:\n",
    "        exc = set(map(_norm_any, EXCLUDE_COUNTRIES))\n",
    "        df = df[\n",
    "            ~(\n",
    "                df[\"country_code\"].map(_norm_any).isin(exc) |\n",
    "                df[\"country_name\"].map(_norm_any).isin(exc) |\n",
    "                df[\"iso3\"].map(_norm_any).isin(exc)\n",
    "            )\n",
    "        ]\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ================= OpenAQ 查询封装（locations/sensors 走 SDK） =================\n",
    "def get_parameter_id(client: OpenAQ, aliases: List[str]) -> int:\n",
    "    \"\"\"按 code/name 小写别名匹配参数ID，例如 ['o3'] 或 ['pm25','pm2.5']\"\"\"\n",
    "    RL.wait()\n",
    "    recs = client.parameters.list(limit=MAX_LIMIT).dict()[\"results\"]\n",
    "    aliases_lc = {a.lower() for a in aliases}\n",
    "    for r in recs:\n",
    "        code = str(r.get(\"code\",\"\")).lower()\n",
    "        name = str(r.get(\"name\",\"\")).lower()\n",
    "        if code in aliases_lc or name in aliases_lc:\n",
    "            return int(r[\"id\"])\n",
    "    raise RuntimeError(f\"未在 OpenAQ 参数列表中找到 {aliases}。\")\n",
    "\n",
    "def list_locations_for_country(client: OpenAQ, country_row: pd.DataFrame, parameters_id: List[int]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    geo_kwargs = {}\n",
    "    if WITHIN_WKT:\n",
    "        geo_kwargs[\"within\"] = WITHIN_WKT\n",
    "        if WITHIN_RADIUS_KM: geo_kwargs[\"radius\"] = float(WITHIN_RADIUS_KM)\n",
    "    if BBOX and isinstance(BBOX,(list,tuple)) and len(BBOX)==4:\n",
    "        geo_kwargs[\"bbox\"] = BBOX\n",
    "\n",
    "    cid = int(country_row[\"countries_id\"])\n",
    "    page = 1\n",
    "    while True:\n",
    "        tries = 0\n",
    "        while True:\n",
    "            try:\n",
    "                RL.wait()\n",
    "                resp = client.locations.list(\n",
    "                    countries_id=cid,\n",
    "                    parameters_id=parameters_id,   # ← 同时过滤 O3 + PM2.5\n",
    "                    page=page, limit=MAX_LIMIT,\n",
    "                    **geo_kwargs\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                tries += 1\n",
    "                cls = _classify_exc(e)\n",
    "                if cls == \"rate\":\n",
    "                    reset = _parse_rate_reset(e) or 25\n",
    "                    print(f\"[RATE] locations {cid} 限流，等待 {reset}s ...\")\n",
    "                    time.sleep(reset + 1); continue\n",
    "                if tries >= RETRY_MAX_TRIES:\n",
    "                    print(f\"[WARN] 国家 {cid} 第{page}页 locations 获取失败，跳过该页。原因: {e}\")\n",
    "                    resp = None; break\n",
    "                _sleep_with_backoff(tries)\n",
    "        if resp is None:\n",
    "            page += 1\n",
    "            if page > 999999: break\n",
    "            continue\n",
    "\n",
    "        results = resp.dict().get(\"results\", [])\n",
    "        for loc in results:\n",
    "            rows.append({\n",
    "                \"countries_id\": cid,\n",
    "                \"country_code\": country_row[\"country_code\"],\n",
    "                \"country_name\": country_row[\"country_name\"],\n",
    "                \"iso3\": country_row.get(\"iso3\"),\n",
    "                \"fasst_region\": country_row.get(\"fasst_region\"),\n",
    "                \"location_id\": int(loc[\"id\"]),\n",
    "                \"location_name\": loc.get(\"name\"),\n",
    "                \"lat\": (loc.get(\"coordinates\") or {}).get(\"latitude\"),\n",
    "                \"lon\": (loc.get(\"coordinates\") or {}).get(\"longitude\"),\n",
    "            })\n",
    "        if len(results) < MAX_LIMIT: break\n",
    "        page += 1; time.sleep(REQUEST_SLEEP)\n",
    "\n",
    "    return pd.DataFrame(rows).drop_duplicates(subset=[\"location_id\"]).reset_index(drop=True)\n",
    "\n",
    "def list_sensors_for_locations(client: OpenAQ, loc_df: pd.DataFrame, target_param_id: int) -> pd.DataFrame:\n",
    "    sensors = []\n",
    "    loc_map = {int(r.location_id): (float(r.lat) if pd.notna(r.lat) else None,\n",
    "                                    float(r.lon) if pd.notna(r.lon) else None)\n",
    "               for _, r in loc_df.iterrows()}\n",
    "\n",
    "    def _run(ids: List[int]) -> List[int]:\n",
    "        retry_these = []\n",
    "        for loc in tqdm(ids, desc=\"sensors per location\", unit=\"loc\"):\n",
    "            tries = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    RL.wait()\n",
    "                    resp = client.locations.sensors(int(loc))  # /v3/locations/{id}/sensors\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    cls = _classify_exc(e)\n",
    "                    if cls == \"notfound\":\n",
    "                        print(f\"[WARN] 站点 {loc} 404，跳过。\")\n",
    "                        resp = None; break\n",
    "                    if cls == \"rate\":\n",
    "                        reset = _parse_rate_reset(e) or 25\n",
    "                        print(f\"[RATE] 站点 {loc} 限流，等待 {reset}s ...\")\n",
    "                        time.sleep(reset + 1); continue\n",
    "                    tries += 1\n",
    "                    if tries >= RETRY_MAX_TRIES:\n",
    "                        print(f\"[WARN] 站点 {loc} 传感器列表失败，进入补捞。原因: {e}\")\n",
    "                        retry_these.append(loc); resp = None; break\n",
    "                    _sleep_with_backoff(tries)\n",
    "            if resp is None: continue\n",
    "            for s in resp.dict().get(\"results\", []):\n",
    "                pid = int((s.get(\"parameter\") or {}).get(\"id\") or -1)\n",
    "                if pid == target_param_id:\n",
    "                    lat, lon = loc_map.get(int(loc), (None, None))\n",
    "                    sensors.append({\n",
    "                        \"location_id\": int(loc),\n",
    "                        \"sensor_id\": int(s[\"id\"]),\n",
    "                        \"parameter_id\": pid,\n",
    "                        \"parameter_name\": (s.get(\"parameter\") or {}).get(\"name\"),\n",
    "                        \"unit\": (s.get(\"parameter\") or {}).get(\"units\"),\n",
    "                        \"lat\": lat, \"lon\": lon,\n",
    "                    })\n",
    "        return retry_these\n",
    "\n",
    "    loc_ids = loc_df[\"location_id\"].astype(int).tolist()\n",
    "    failed = _run(loc_ids)\n",
    "    if failed:\n",
    "        print(f\"[INFO] sensors 补捞轮，待补 {len(failed)} 个站点 ...\")\n",
    "        time.sleep(2); _ = _run(failed)\n",
    "\n",
    "    return pd.DataFrame(sensors)\n",
    "\n",
    "\n",
    "# --------- 小时数据抓取：/v3/sensors/{id}/hours ---------\n",
    "def _iso(dt: datetime) -> str:\n",
    "    return dt.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def _gen_chunks(dt_from: datetime, dt_to: datetime, days: int) -> List[Tuple[datetime, datetime]]:\n",
    "    out, cur, delta = [], dt_from, timedelta(days=days)\n",
    "    while cur < dt_to:\n",
    "        nxt = min(cur + delta, dt_to)\n",
    "        out.append((cur, nxt)); cur = nxt\n",
    "    return out\n",
    "\n",
    "def _calc_total_pages(meta: dict | None, per_page: int) -> Optional[int]:\n",
    "    \"\"\"从响应 meta 安全计算总页数；meta.found 可能为数值/字符串/缺失。\"\"\"\n",
    "    if not meta or per_page <= 0:\n",
    "        return None\n",
    "    found_raw = meta.get(\"found\", None)\n",
    "    if found_raw is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(found_raw, (int, float)):\n",
    "            found_int = int(found_raw)\n",
    "        else:\n",
    "            found_int = int(re.sub(r\"[^\\d]\", \"\", str(found_raw)))  # 去掉逗号等分隔符\n",
    "        if found_int <= 0:\n",
    "            return 0\n",
    "        return math.ceil(found_int / float(per_page))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _http_get_json(session: requests.Session, url: str, params: Dict) -> Dict:\n",
    "    RL.wait()\n",
    "    resp = session.get(url, params=params, timeout=REQUEST_TIMEOUT)\n",
    "    if resp.status_code == 429:\n",
    "        ra = _parse_rate_reset_from_resp(resp) or 25\n",
    "        time.sleep(ra + 1); RL.wait()\n",
    "        resp = session.get(url, params=params, timeout=REQUEST_TIMEOUT)\n",
    "    if resp.status_code >= 400:\n",
    "        resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def _quick_count_hours(session: requests.Session, sid: int, dt_from: datetime, dt_to: datetime) -> Optional[int]:\n",
    "    \"\"\"快速预检某传感器在窗口内的总条数（limit=1）。返回 0/正整数/None(未知)。\"\"\"\n",
    "    url = f\"{BASE_URL}/sensors/{sid}/hours\"\n",
    "    params = {\n",
    "        \"limit\": 1, \"page\": 1,\n",
    "        \"date_from\": _iso(dt_from), \"date_to\": _iso(dt_to),\n",
    "        \"order_by\": \"datetime\", \"sort_order\": \"asc\",\n",
    "    }\n",
    "    tries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            j = _http_get_json(session, url, params)\n",
    "            meta = j.get(\"meta\", {}) or {}\n",
    "            total_pages = _calc_total_pages(meta, 1)\n",
    "            if total_pages is None:\n",
    "                # meta 不提供 found；以首批结果估计\n",
    "                return len(j.get(\"results\", []) or [])\n",
    "            # pages == found（因为 per_page=1）\n",
    "            return int(total_pages)\n",
    "        except Exception as e:\n",
    "            tries += 1\n",
    "            cls = _classify_exc(e)\n",
    "            if cls == \"rate\":\n",
    "                reset = _parse_rate_reset(e) or 25\n",
    "                print(f\"[RATE] 预检 {sid} 限流，等待 {reset}s ...\")\n",
    "                time.sleep(reset + 1); continue\n",
    "            if tries >= RETRY_MAX_TRIES:\n",
    "                print(f\"[WARN] 预检 {sid} 失败，跳过预估：{e}\")\n",
    "                return None\n",
    "            _sleep_with_backoff(tries)\n",
    "\n",
    "def _fetch_one_chunk_to_csv(session: requests.Session, sid: int, loc: int,\n",
    "                            chunk_from: datetime, chunk_to: datetime,\n",
    "                            csv_path: str, wrote_header_flag: List[bool]) -> int:\n",
    "    url = f\"{BASE_URL}/sensors/{sid}/hours\"\n",
    "    total_rows = 0\n",
    "    for lim in PAGE_LIMITS_TRY:\n",
    "        page, bad = 1, 0\n",
    "        while True:\n",
    "            tries = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    params = {\n",
    "                        \"limit\": lim, \"page\": page,\n",
    "                        \"date_from\": _iso(chunk_from), \"date_to\": _iso(chunk_to),\n",
    "                        \"order_by\": \"datetime\", \"sort_order\": \"asc\",\n",
    "                    }\n",
    "                    j = _http_get_json(session, url, params)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    tries += 1\n",
    "                    cls = _classify_exc(e)\n",
    "                    if cls == \"rate\":\n",
    "                        reset = _parse_rate_reset(e) or 25\n",
    "                        print(f\"[RATE] sid={sid} chunk {chunk_from:%F}->{chunk_to:%F} 等待 {reset}s ...\")\n",
    "                        time.sleep(reset + 1); continue\n",
    "                    if tries >= RETRY_MAX_TRIES:\n",
    "                        bad += 1; break\n",
    "                    _sleep_with_backoff(tries)\n",
    "\n",
    "            if tries >= RETRY_MAX_TRIES:\n",
    "                page += 1\n",
    "                if bad >= 3: break\n",
    "                continue\n",
    "\n",
    "            results = j.get(\"results\", []) or []\n",
    "            if results:\n",
    "                batch = []\n",
    "                for m in results:\n",
    "                    val = m.get(\"value\")\n",
    "                    if val is None and isinstance(m.get(\"summary\"), dict):\n",
    "                        val = m[\"summary\"].get(\"avg\")\n",
    "\n",
    "                    ts = None\n",
    "                    dt_field = m.get(\"datetime\")\n",
    "                    if isinstance(dt_field, dict):\n",
    "                        ts = dt_field.get(\"utc\") or dt_field.get(\"value\") or dt_field.get(\"local\")\n",
    "                    elif dt_field:\n",
    "                        ts = dt_field\n",
    "                    if not ts:\n",
    "                        period = m.get(\"period\") or {}\n",
    "                        if isinstance(period, dict):\n",
    "                            dt_from = period.get(\"datetimeFrom\") or period.get(\"from\") or {}\n",
    "                            if isinstance(dt_from, dict):\n",
    "                                ts = dt_from.get(\"utc\") or dt_from.get(\"local\")\n",
    "                            elif dt_from:\n",
    "                                ts = dt_from\n",
    "                    if not ts:\n",
    "                        coverage = m.get(\"coverage\") or {}\n",
    "                        if isinstance(coverage, dict):\n",
    "                            dt_from = coverage.get(\"datetimeFrom\") or {}\n",
    "                            if isinstance(dt_from, dict):\n",
    "                                ts = dt_from.get(\"utc\") or dt_from.get(\"local\")\n",
    "                            elif dt_from:\n",
    "                                ts = dt_from\n",
    "                    if not ts:\n",
    "                        dt_candidate = m.get(\"date\") or m.get(\"timestamp\")\n",
    "                        if isinstance(dt_candidate, dict):\n",
    "                            ts = dt_candidate.get(\"utc\") or dt_candidate.get(\"local\")\n",
    "                        elif dt_candidate:\n",
    "                            ts = dt_candidate\n",
    "\n",
    "                    if ts is None or val is None:\n",
    "                        continue\n",
    "\n",
    "                    ts_parsed = pd.to_datetime(ts, utc=True, errors=\"coerce\")\n",
    "                    if pd.isna(ts_parsed):\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        valf = float(val)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    unit = m.get(\"unit\")\n",
    "                    if not unit and isinstance(m.get(\"parameter\"), dict):\n",
    "                        unit = m[\"parameter\"].get(\"units\") or m[\"parameter\"].get(\"unit\")\n",
    "\n",
    "                    batch.append({\n",
    "                        \"sensor_id\": sid,\n",
    "                        \"location_id\": loc,\n",
    "                        \"datetime_utc\": ts_parsed.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                        \"value\": valf,\n",
    "                        \"unit\": unit,\n",
    "                    })\n",
    "                if batch:\n",
    "                    dfb = pd.DataFrame(batch)\n",
    "                    dfb.to_csv(csv_path, mode=\"a\", index=False, header=(not wrote_header_flag[0]))\n",
    "                    wrote_header_flag[0] = True\n",
    "                    total_rows += len(dfb)\n",
    "\n",
    "            meta = j.get(\"meta\", {}) or {}\n",
    "            total_pages = _calc_total_pages(meta, lim)\n",
    "            if total_pages is not None and page >= total_pages:\n",
    "                break\n",
    "            if len(results) < lim:\n",
    "                break\n",
    "            page += 1; time.sleep(REQUEST_SLEEP)\n",
    "\n",
    "        if total_rows > 0:\n",
    "            return total_rows\n",
    "    return -1\n",
    "\n",
    "def _fetch_chunk_recursive(session: requests.Session, sid: int, loc: int,\n",
    "                           chunk_from: datetime, chunk_to: datetime,\n",
    "                           csv_path: str, wrote_header_flag: List[bool],\n",
    "                           depth: int) -> int:\n",
    "    rows = _fetch_one_chunk_to_csv(session, sid, loc, chunk_from, chunk_to, csv_path, wrote_header_flag)\n",
    "    if rows >= 0: return max(rows, 0)\n",
    "    if depth >= MAX_CHUNK_SPLIT_DEPTH: return 0\n",
    "    mid = chunk_from + (chunk_to - chunk_from) / 2\n",
    "    print(f\"  ↪ sid={sid} 分裂块: {chunk_from:%F}->{mid:%F} / {mid:%F}->{chunk_to:%F}\")\n",
    "    left = _fetch_chunk_recursive(session, sid, loc, chunk_from, mid, csv_path, wrote_header_flag, depth + 1)\n",
    "    right = _fetch_chunk_recursive(session, sid, loc, mid, chunk_to, csv_path, wrote_header_flag, depth + 1)\n",
    "    return left + right\n",
    "\n",
    "def fetch_hours_stream_to_csv(\n",
    "    sensors_df: pd.DataFrame,\n",
    "    csv_path: str,\n",
    "    datetime_from: Optional[str] = None,\n",
    "    datetime_to: Optional[str] = None,\n",
    "    desc: str = \"fetch /sensors/{id}/hours\"\n",
    "):\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    if os.path.exists(csv_path): os.remove(csv_path)\n",
    "    wrote_header_flag = [False]\n",
    "\n",
    "    if sensors_df.empty:\n",
    "        pd.DataFrame(columns=[\"sensor_id\",\"location_id\",\"datetime_utc\",\"value\",\"unit\"]).to_csv(csv_path, index=False)\n",
    "        return 0\n",
    "\n",
    "    dt_to = pd.to_datetime(datetime_to, utc=True).to_pydatetime()\n",
    "    dt_from = pd.to_datetime(datetime_from, utc=True).to_pydatetime()\n",
    "\n",
    "    session = _build_http_session(API_KEY)\n",
    "    total_rows = 0\n",
    "\n",
    "    # 预检：先判断每个传感器在整个窗口内是否有数据\n",
    "    jobs = []\n",
    "    print(f\"[预检] 估算每个传感器在 {dt_from:%F} ~ {dt_to:%F} 的数据条数（limit=1） ...\")\n",
    "    for _, r in sensors_df.iterrows():\n",
    "        sid = int(r[\"sensor_id\"]); loc = int(r[\"location_id\"])\n",
    "        cnt = _quick_count_hours(session, sid, dt_from, dt_to)\n",
    "        jobs.append((sid, loc, cnt))\n",
    "        if cnt is not None:\n",
    "            print(f\"  - sid={sid} 预估条数≈{cnt}\")\n",
    "        else:\n",
    "            print(f\"  - sid={sid} 预估条数未知（继续抓取）\")\n",
    "    # 跳过明确为 0 的\n",
    "    jobs = [(sid,loc,cnt) for (sid,loc,cnt) in jobs if (cnt is None or cnt > 0)]\n",
    "    if not jobs:\n",
    "        pd.DataFrame(columns=[\"sensor_id\",\"location_id\",\"datetime_utc\",\"value\",\"unit\"]).to_csv(csv_path, index=False)\n",
    "        session.close(); return 0\n",
    "\n",
    "    pbar = tqdm(jobs, desc=desc, unit=\"sensor\")\n",
    "    for sid, loc, cnt in pbar:\n",
    "        print(f\"[sid={sid}] 预计条数≈{cnt if cnt is not None else '未知'}，开始分块抓取 ...\")\n",
    "        chunks = _gen_chunks(dt_from, dt_to, HOURS_CHUNK_DAYS)\n",
    "        rows_this_sensor = 0\n",
    "        for i, (c_from, c_to) in enumerate(chunks, 1):\n",
    "            print(f\"  · chunk {i}/{len(chunks)}: {c_from:%F} → {c_to:%F}\")\n",
    "            rows_this_sensor += _fetch_chunk_recursive(\n",
    "                session, sid, loc, c_from, c_to, csv_path, wrote_header_flag, depth=0\n",
    "            )\n",
    "        total_rows += rows_this_sensor\n",
    "        print(f\"[sid={sid}] 完成，新增 {rows_this_sensor} 行。\")\n",
    "\n",
    "    session.close()\n",
    "    return total_rows\n",
    "\n",
    "\n",
    "# ========================== CSV 自检 ==========================\n",
    "def verify_csv(csv_path: str, print_rows: int = 5) -> None:\n",
    "    \"\"\"基本完整性检查：列存在、时间可解析、去重后行数、样例预览。\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"[VERIFY] 文件不存在：{csv_path}\"); return\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required = [\"sensor_id\",\"location_id\",\"datetime_utc\",\"value\",\"unit\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[VERIFY] 缺少列：{missing}\"); return\n",
    "\n",
    "    # 解析时间并去重（同一 sensor + 时间）\n",
    "    df[\"datetime_utc\"] = pd.to_datetime(df[\"datetime_utc\"], utc=True, errors=\"coerce\")\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"datetime_utc\",\"value\"])\n",
    "    df = df.drop_duplicates(subset=[\"sensor_id\",\"datetime_utc\"])\n",
    "    after = len(df)\n",
    "\n",
    "    nunique_sensors = df[\"sensor_id\"].nunique()\n",
    "    time_min = df[\"datetime_utc\"].min()\n",
    "    time_max = df[\"datetime_utc\"].max()\n",
    "    print(f\"[VERIFY] ✅ CSV OK | 行数(去重后): {after}/{before} | 传感器数: {nunique_sensors} | 时间范围: {time_min} ~ {time_max}\")\n",
    "    print(df.head(print_rows).to_string(index=False))\n",
    "\n",
    "\n",
    "# ========================== 主流程 ==========================\n",
    "def main():\n",
    "    if not API_KEY or API_KEY.startswith(\"PUT-\"):\n",
    "        raise SystemExit(\"请先配置 OPENAQ_API_KEY（v3 必须有 Key）。\")\n",
    "\n",
    "    _maybe_disable_env_proxies()\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # 时间窗\n",
    "    dt_from_s = _parse_iso_z(DATETIME_FROM)\n",
    "    dt_to_s   = _parse_iso_z(DATETIME_TO)\n",
    "\n",
    "    print(\"读取国家 CSV ...\")\n",
    "    regions_df = load_regions_csv(COUNTRY_CSV_PATH, COUNTRY_CSV_COLUMNS)\n",
    "\n",
    "    client = OpenAQ(api_key=API_KEY)\n",
    "    try:\n",
    "        print(\"映射国家 → OpenAQ countries_id ...\")\n",
    "        country_map = map_countries_to_openaq_ids(client, regions_df)\n",
    "        if country_map.empty:\n",
    "            raise RuntimeError(\"Country CSV rows could not be mapped to OpenAQ countries.\")\n",
    "        print(f\"国家数：{len(country_map)}\")\n",
    "\n",
    "        print(\"获取参数ID ...\")\n",
    "        o3_id = get_parameter_id(client, [\"o3\",\"ozone\"])\n",
    "        pm25_id = get_parameter_id(client, [\"pm25\",\"pm2.5\"])  # 也可直接写 2，但稳妥起见动态获取\n",
    "\n",
    "        for idx, crow in country_map.iterrows():\n",
    "            cc = crow[\"country_code\"]; cname = crow[\"country_name\"]\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"[{idx+1}/{len(country_map)}] 国家 {cname} ({cc}) — 列出含 O3/PM2.5 的站点 ...\")\n",
    "\n",
    "            # 1) 列出该国含 O3 或 PM2.5 的站点\n",
    "            loc_df = list_locations_for_country(client, crow, [o3_id, pm25_id])\n",
    "            loc_out = os.path.join(OUTPUT_DIR, f\"{cc}_site_catalog.csv\")\n",
    "            loc_df.to_csv(loc_out, index=False)\n",
    "            print(f\"[OK] 站点清单：{loc_out}（{len(loc_df)} 个）\")\n",
    "\n",
    "            if loc_df.empty:\n",
    "                for pol in (\"o3\",\"pm25\"):\n",
    "                    hourly_path = os.path.join(OUTPUT_DIR, f\"{cc}_hourly_{pol}.csv\")\n",
    "                    pd.DataFrame(columns=[\"sensor_id\",\"location_id\",\"datetime_utc\",\"value\",\"unit\"]).to_csv(hourly_path, index=False)\n",
    "                print(f\"[PAUSE] 国家间暂停 {PER_COUNTRY_PAUSE_SEC}s ...\")\n",
    "                time.sleep(PER_COUNTRY_PAUSE_SEC); continue\n",
    "\n",
    "            # 2) 每国按站点抽样（为了公平，不区分参数先抽样再筛传感器）\n",
    "            if MAX_LOCATIONS_PER_COUNTRY and MAX_LOCATIONS_PER_COUNTRY > 0 and len(loc_df) > MAX_LOCATIONS_PER_COUNTRY:\n",
    "                loc_df = loc_df.sample(n=MAX_LOCATIONS_PER_COUNTRY, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            # 3) 列出 O3 与 PM2.5 的传感器（把 lat/lon 带上）\n",
    "            print(f\"[{cc}] 列出 O3 传感器 ...\")\n",
    "            o3_sensors_df = list_sensors_for_locations(client, loc_df, o3_id)\n",
    "            print(f\"[{cc}] 列出 PM2.5 传感器 ...\")\n",
    "            pm25_sensors_df = list_sensors_for_locations(client, loc_df, pm25_id)\n",
    "\n",
    "            # 4) 逐小时抓取与写出\n",
    "            hourly_o3 = os.path.join(OUTPUT_DIR, f\"{cc}_hourly_o3.csv\")\n",
    "            hourly_pm25 = os.path.join(OUTPUT_DIR, f\"{cc}_hourly_pm25.csv\")\n",
    "\n",
    "            if not o3_sensors_df.empty:\n",
    "                print(f\"[{cc}] 抓取逐小时 O3 并写入：{hourly_o3} ...\")\n",
    "                nrows_o3 = fetch_hours_stream_to_csv(\n",
    "                    o3_sensors_df, hourly_o3, datetime_from=dt_from_s, datetime_to=dt_to_s,\n",
    "                    desc=\"fetch /sensors/{id}/hours (O3)\"\n",
    "                )\n",
    "                print(f\"[OK] {cc} O3 写入 {nrows_o3} 条小时数据。\")\n",
    "                verify_csv(hourly_o3)\n",
    "            else:\n",
    "                pd.DataFrame(columns=[\"sensor_id\",\"location_id\",\"datetime_utc\",\"value\",\"unit\"]).to_csv(hourly_o3, index=False)\n",
    "\n",
    "            if not pm25_sensors_df.empty:\n",
    "                print(f\"[{cc}] 抓取逐小时 PM2.5 并写入：{hourly_pm25} ...\")\n",
    "                nrows_pm25 = fetch_hours_stream_to_csv(\n",
    "                    pm25_sensors_df, hourly_pm25, datetime_from=dt_from_s, datetime_to=dt_to_s,\n",
    "                    desc=\"fetch /sensors/{id}/hours (PM2.5)\"\n",
    "                )\n",
    "                print(f\"[OK] {cc} PM2.5 写入 {nrows_pm25} 条小时数据。\")\n",
    "                verify_csv(hourly_pm25)\n",
    "            else:\n",
    "                pd.DataFrame(columns=[\"sensor_id\",\"location_id\",\"datetime_utc\",\"value\",\"unit\"]).to_csv(hourly_pm25, index=False)\n",
    "\n",
    "            # 5) 国家之间暂停（30s）\n",
    "            print(f\"[PAUSE] 国家间暂停 {PER_COUNTRY_PAUSE_SEC}s ...\")\n",
    "            time.sleep(PER_COUNTRY_PAUSE_SEC)\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"[DONE] 全部国家处理完成。所有 CSV 已写入 output/\")\n",
    "\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}